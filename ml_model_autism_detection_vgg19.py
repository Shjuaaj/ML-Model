# -*- coding: utf-8 -*-
"""ml_model_autism_detection_VGG19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18bqOua0M2oYaqKYdSZIiH4ftF0QSbk3V
"""

from google.colab import drive
drive.mount('/content/drive')

"""4/1AfDhmrjYTr3RqIIYJi-xLan60WG_kwSkb3SRSrEv53VQx24A8UoYHjKZMZ0"""

! pip install -q keras

import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

import keras

import numpy as np
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os
#print(os.listdir("../drive"))
print(os.listdir("/content/drive/My Drive/AutismDataset"))

import keras,os
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D
import numpy as np
from keras.applications import VGG19
from keras.models import Model
from keras import optimizers , layers, applications

"""###Train Dataset"""

filenames = os.listdir("/content/drive/My Drive/AutismDataset/train")
categories = []
for filename in filenames:
    #print("opudj")
    category = filename.split('.')[0]
    if category == 'Autistic':
        categories.append(str(1))
    else:
        categories.append(str(0))

train_df = pd.DataFrame({
    'filename': filenames,
    'category': categories
})
print(train_df)

train_df.head()

train_df['category'].value_counts().plot.bar()

"""### Test dataset"""

test_filenames = os.listdir("/content/drive/My Drive/AutismDataset/test")
test_df = pd.DataFrame({
    'filename': test_filenames
})
print(test_df)

test_df.head()

from PIL import Image
import random

"""### Sample Image"""

sample = random.choice(filenames)
image = load_img("/content/drive/My Drive/AutismDataset/train/"+sample)
plt.imshow(image)

"""###Preparing Model"""

image_size = 224
input_shape = (image_size, image_size, 3)

#Hyperparameters
epochs = 10
batch_size = 12
pre_trained_model = VGG19(input_shape=input_shape, include_top=False, weights="imagenet")
last_layer = pre_trained_model.get_layer('block5_pool')
last_output = last_layer.output

# Flatten the output layer to 1 dimension
x = GlobalMaxPooling2D()(last_output)

# Add a fully connected layer with 512 hidden units and ReLU activation
x = Dense(512, activation='relu')(x)

# Add a dropout rate of 0.5
x = Dropout(0.5)(x)

# Add a final sigmoid layer for classification
x = layers.Dense(1, activation='sigmoid')(x)

#transfer learning
model = Model(pre_trained_model.input, x)

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.SGD(lr=1e-3, momentum=0.9),
              metrics=['accuracy'])

model.summary()

"""###Prepare Test and Train Data

"""

train_df, validate_df = train_test_split(train_df, test_size=0.1)
train_df = train_df.reset_index()
validate_df = validate_df.reset_index()

total_train = train_df.shape[0]
total_validate = validate_df.shape[0]

"""###Training Generator & *Preprocessing*

"""

train_datagen = ImageDataGenerator(
    rotation_range=15,
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    width_shift_range=0.1,
    height_shift_range=0.1
)

train_generator = train_datagen.flow_from_dataframe(
    train_df,
    "/content/drive/My Drive/AutismDataset/train",
    x_col='filename',
    y_col='category',
    class_mode='binary',
    target_size=(image_size, image_size),
    batch_size=batch_size
)
x_batch, y_batch = next(train_generator)
for i in range (0,5):
    image = x_batch[i]
    plt.imshow(image)
    plt.show()

"""###Validation Generator"""

validation_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = validation_datagen.flow_from_dataframe(
    validate_df,
    "/content/drive/My Drive/AutismDataset/train",
    x_col='filename',
    y_col='category',
    class_mode='binary',
    target_size=(image_size, image_size),
    batch_size=batch_size
)
print(validation_generator.class_indices)
x_batch, y_batch = next(validation_generator)
for i in range (0,5):
    image = x_batch[i]
    plt.imshow(image)
    plt.show()

"""###Fit Model"""

history = model.fit_generator(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size)

loss, accuracy = model.evaluate_generator(validation_generator, total_validate//batch_size, workers=12)
print("Test: accuracy = %f  ;  loss = %f " % (accuracy, loss))

"""###Prepare Testing"""

test_filenames = os.listdir("/content/drive/My Drive/AutismDataset/test/")
test_df = pd.DataFrame({
    'filename': test_filenames
})
nb_samples = test_df.shape[0]

"""###Create Testing Generator

"""

test_gen = ImageDataGenerator(rescale=1./255)
test_generator = test_gen.flow_from_dataframe(
    test_df,
    "/content/drive/My Drive/AutismDataset/test/",
    x_col='filename',
    y_col=None,
    class_mode=None,
    batch_size=batch_size,
    target_size=(image_size, image_size),
    shuffle=False
)
x_batch= next(test_generator)
for i in range (0,5):
    image = x_batch[i]
    plt.imshow(image)
    plt.show()

"""# Prediction :"""

threshold = 0.5
predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))
threshold = 0.5
test_df['category'] = np.where(predict > threshold, 1,0)

"""Image name as :- actual_name(prediction {0 or 1}) , eg. autistic.127.jpg(1)

###Predicted Result
"""

#See Predicted Result
sample_test = test_df.sample(n=9).reset_index()
sample_test.head()
plt.figure(figsize=(12, 12))
for index, row in sample_test.iterrows():
    filename = row['filename']
    category = row['category']
    img = load_img("/content/drive/My Drive/AutismDataset/test/"+filename, target_size=(256, 256))
    plt.subplot(3, 3, index+1)
    plt.imshow(img)
    plt.xlabel(filename + '(' + "{}".format(category) + ')')
plt.tight_layout()
plt.show()

"""#Results and Analysis :"""

import seaborn as sns

"""### Predicted:"""

submission_df = test_df.copy()
submission_df['id'] = submission_df['filename'].str.split('.').str[0]
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('submission_13010030.csv', index=False)

"""###CSV file OUTPUT"""

import pandas as pd
pd.read_csv('submission_13010030.csv')

plt.figure(figsize=(10,5))
sns.countplot(submission_df['label'])
plt.title("(Predicted data)")

"""### ACTUAL:"""

submission_df = test_df.copy()
submission_df['id'] = submission_df['filename'].str.split('.').str[0]
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('submission_13010030.csv', index=False)


plt.figure(figsize=(10,5))
sns.countplot(submission_df['id'])
plt.title("(Test data)")

"""### Data results: (predicted) :"""

import csv



my_reader = csv.reader(open('submission_13010030.csv'))
predicted_autistic = 0
for record in my_reader:
    if record[1] == '1':
        predicted_autistic += 1
print("predicted Autistic : " ,predicted_autistic)

my_reader = csv.reader(open('submission_13010030.csv'))
predicted_non_autistic = 0
for record1 in my_reader:
    if record1[1] == '0':
        predicted_non_autistic += 1
print("predicted Non Autistic : " ,predicted_non_autistic)

my_reader = csv.reader(open('submission_13010030.csv'))
autistic = 0
for record1 in my_reader:
    if record1[0] == 'Autistic':
        autistic += 1
print("Actual Autistic : " ,autistic)

my_reader = csv.reader(open('submission_13010030.csv'))
non_autistic = 0
for record1 in my_reader:
    if record1[0] == 'Non_Autistic':
        non_autistic += 1
print("Actual Non Autistic : " ,non_autistic)

#accuracy for predicting
print("Actual Non Autistic percentage in total test data: " ,(non_autistic/300)*100,"%")
print("Predicted Non Autistic percentage in total test data: " ,(predicted_non_autistic/300)*100,"%")
print("Actual Autistic percentage in total test data: " ,(autistic/300)*100,"%")

print("Predicted Autistic percentage in total test data: " ,(predicted_autistic/300)*100,"%")

"""###Confusion Matrix"""

my_reader = csv.reader(open('submission_13010030.csv'))
true_pos = 0 #autistic,1
for record1 in my_reader:
    if record1[0] == 'Autistic' and record1[1]=='1':
        true_pos += 1
print("True positive : " ,true_pos)

my_reader = csv.reader(open('submission_13010030.csv'))
true_neg = 0 #non_autistic,0
for record1 in my_reader:
    if record1[0] == 'Non_Autistic' and record1[1]=='0':
        true_neg += 1
print("True Negative : " ,true_neg)

my_reader = csv.reader(open('submission_13010030.csv'))
false_pos = 0 #autistic,0
for record1 in my_reader:
    if record1[0] == 'Autistic' and record1[1]=='0':
       false_pos += 1
print("false Positive : " ,false_pos)

my_reader = csv.reader(open('submission_13010030.csv'))
false_neg = 0 #non_autistic,1
for record1 in my_reader:
    if record1[0] == 'Non_Autistic' and record1[1]=='1':
       false_neg += 1
print("false Negative : " ,false_neg)

"""###Accuracy"""

accuracy = (true_pos + true_neg)/(true_pos + true_neg + false_pos + false_neg)
print("Accuracy is: ",accuracy*100,"%")

"""###Precision"""

precision = true_pos / ( true_pos + false_pos)
print("Precision is: ",precision*100,"%")

"""###Sensitivity"""

sensitivity = true_pos / (true_pos + false_neg)
print("Sensitivity is: ",sensitivity*100,"%")

"""###Specificity"""

Specificity =true_neg / (true_neg + false_pos)
print("Specificity is: ",Specificity*100,"%")

"""# Result on OUR FACES (TAKING REAL DATASET)

### Loading Testing data
"""

test_filenames2 = os.listdir("/content/drive/My Drive/AutismDataset/test2")
test_df2 = pd.DataFrame({
    'filename': test_filenames2
})
print(test_df2)

nb_samples2 = test_df2.shape[0]

"""### preparing Testing Generator"""

test_gen2 = ImageDataGenerator(rescale=1./255)
test_generator2 = test_gen.flow_from_dataframe(
    test_df2,
    "/content/drive/My Drive/AutismDataset/test2/",
    x_col='filename',
    y_col=None,
    class_mode=None,
    batch_size=batch_size,
    target_size=(image_size, image_size),
    shuffle=False
)
x_batch2= next(test_generator2)
for i in range (0,3):
    image = x_batch2[i]
    plt.imshow(image)
    plt.show()

"""### Prediction"""

threshold = 0.5
predict2 = model.predict_generator(test_generator2, steps=np.ceil(nb_samples2/batch_size))
test_df2['category'] = np.where(predict2 > threshold, 1,0)

"""### Prediction Result"""

#See Predicted Result
sample_test2 = test_df2.sample(n=4).reset_index()
sample_test2.head()
plt.figure(figsize=(12, 12))
for index, row in sample_test2.iterrows():
    filename = row['filename']
    category = row['category']
    img = load_img("/content/drive/My Drive/AutismDataset/test2/"+filename, target_size=(256, 256))
    plt.subplot(3, 3, index+1)
    plt.imshow(img)
    plt.xlabel(filename + '(' + "{}".format(category) + ')')
plt.tight_layout()
plt.show()

submission_df = test_df2.copy()
submission_df['id'] = submission_df['filename'].str.split('.').str[0]
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('submission_130100301.csv', index=False)

import pandas as pd
pd.read_csv('submission_130100301.csv')